{% set cov_enabled = (cov_enabled == "1") %}
{% set docker_image = "public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT" %}
{% set pull_through_docker_image = "936637512419.dkr.ecr.us-west-2.amazonaws.com/vllm-ci-pull-through-cache/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT" %}
{% set docker_image_torch_nightly = "public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT-torch-nightly" %}
{% set docker_image_cpu = "public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT-cpu" %}
{% set docker_image_arm64_cpu = "public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT-arm64-cpu" %}
{% if branch == "main" %}
{% set docker_image = "public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:$BUILDKITE_COMMIT" %}
{% set pull_through_docker_image = "936637512419.dkr.ecr.us-west-2.amazonaws.com/vllm-ci-pull-through-cache/q9t5s3a7/vllm-ci-postmerge-repo:$BUILDKITE_COMMIT" %}
{% set docker_image_latest = "public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:latest" %}
{% set docker_image_torch_nightly = "public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:$BUILDKITE_COMMIT-torch-nightly" %}
{% set docker_image_cpu = "public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:$BUILDKITE_COMMIT-cpu" %}
{% set docker_image_arm64_cpu = "public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:$BUILDKITE_COMMIT-arm64-cpu" %}
{% set docker_image_hpu = "public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:$BUILDKITE_COMMIT-hpu" %}
{% endif %}
{% set docker_image_amd = "rocm/vllm-ci:$BUILDKITE_COMMIT" %}
{% set default_working_dir = "/vllm-workspace/tests" %}
{% set hf_home = "/root/.cache/huggingface" %}
{% set hf_home_efs = "/mnt/efs/hf_cache" %}
{% set hf_home_fsx = "/fsx/hf_cache" %}
{% set list_file_diff = list_file_diff | split("|") %}

{% macro add_pytest_coverage(cmd, coverage_file) %}
{% if "pytest " in cmd %}
COVERAGE_FILE={{ coverage_file }} {{ cmd | replace("pytest ", "pytest --cov=vllm --cov-report= --cov-append --durations=0 ") }} || true
{% else %}
{{ cmd }}
{% endif %}
{% endmacro %}

{% macro add_docker_pytest_coverage(step, cov_enabled) %}
{# Compute coverage file id #}
{% set step_length = step.label | length %}
{% set step_first = step.label | first | default("x") %}
{% set coverage_file = ".coverage." + step_length ~ "_" ~ step_first %}

{% if cov_enabled %}
{% set ns = namespace(has_pytest=false) %}
{% if step.command %}
{% if "pytest " in step.command %}{% set ns.has_pytest = true %}{% endif %}
{{ add_pytest_coverage(step.command, coverage_file) }}
{% else %}
{% for cmd in step.commands %}
{% if "pytest " in cmd %}{% set ns.has_pytest = true %}{% endif %}
{{ add_pytest_coverage(cmd, coverage_file) }}{{ " && " if not loop.last else "" }}{% endfor %}
{% endif %}{% if ns.has_pytest %} && curl -sSL https://raw.githubusercontent.com/vllm-project/ci-infra/{{ vllm_ci_branch | default('main') }}/buildkite/scripts/upload_codecov.sh | bash -s -- \"{{ step.label }}\"{% endif %}
{% else %}
{{ step.command or (step.commands | join(' && ')) | safe }}
{% endif %}
{% endmacro %}

{% macro render_cuda_config(step, image, default_working_dir, hf_home_fsx, hf_home, branch) %}
agents:
  {% if step.label == "Documentation Build" %}
  queue: small_cpu_queue_premerge
  {% elif step.no_gpu %}
  queue: cpu_queue_premerge_us_east_1
  {% elif step.gpu == "a100" %}
  queue: a100_queue
  {% elif step.gpu == "h100" %}
  queue: mithril-h100-pool
  {% elif step.gpu == "h200" and step.num_gpus and step.num_gpus == 8 %}
  queue: nebius-h200
  {% elif step.gpu == "h200" %}
  queue: skylab-h200
  {% elif step.gpu == "b200" %}
  queue: B200
  {% elif step.num_gpus == 2 or step.num_gpus == 4 %}
  queue: gpu_4_queue
  {% else %}
  queue: gpu_1_queue
  {% endif %}

{% if step.num_nodes >= 2 %}
commands:
  - ./.buildkite/scripts/run-multi-node-test.sh {{ (step.working_dir or default_working_dir) | safe }} {{ step.num_nodes }} {{ step.num_gpus }} {{ image }} {% for command in step.commands %}"{{ (command | join(' && ')) | safe }}" {% endfor %}
{% endif %}

{% if step.parallelism %}
parallelism: {{ step.parallelism }}
{% endif %}

retry:
  automatic:
    - exit_status: -1
      limit: 1
    - exit_status: -10
      limit: 1

{% if step.num_nodes < 2 %}
plugins:
  {% if step.gpu != "a100" and step.gpu != "h100" and step.gpu != "h200" and step.gpu != "b200" %}
  - docker#v5.2.0:
      image: {{ image }}
      always-pull: true
      propagate-environment: true
      {% if not step.no_gpu %}
      gpus: all
      {% endif %}
      {% if step.label == "Benchmarks" or step.mount_buildkite_agent or cov_enabled %}
      mount-buildkite-agent: true
      {% endif %}
      command: ["bash", "{% if fail_fast == "true" %}-xce{% else %}-xc{% endif %}", "(command nvidia-smi || true) && export CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 && export CUDA_COREDUMP_SHOW_PROGRESS=1 && export CUDA_COREDUMP_GENERATION_FLAGS='skip_nonrelocated_elf_images,skip_global_memory,skip_shared_memory,skip_local_memory,skip_constbank_memory' && cd {{ (step.working_dir or default_working_dir) | safe }} && {{ add_docker_pytest_coverage(step, cov_enabled) }}"]
      environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME={{ hf_home_fsx }}
        - HF_TOKEN
        - CODECOV_TOKEN
        {% if fail_fast == "true" %}
        - PYTEST_ADDOPTS=-x
        {% endif %}
        {% if branch == "main" %}
        - BUILDKITE_ANALYTICS_TOKEN
        {% endif %}
        {% if step.label == "Speculative decoding tests" %}
        - VLLM_ATTENTION_BACKEND=XFORMERS
        {% endif %}
      volumes:
        - /dev/shm:/dev/shm
        - {{ hf_home_fsx }}:{{ hf_home_fsx }}
  {% elif step.gpu == "h200" and step.num_gpus and step.num_gpus == 8 %}
  - kubernetes:
      podSpec:
        containers:
          - image: {{ image }}
            command:
              - bash -c "{{ '(command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1 && cd ' ~ ((step.working_dir or default_working_dir) | safe) ~ ' && ' ~ (step.command or (step.commands | join(" && ")) | safe) }}"
            resources:
              limits:
                nvidia.com/gpu: {{ step.num_gpus or 1 }}
            volumeMounts:
              - name: devshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: {{ hf_home }}
            env:
              - name: VLLM_USAGE_SOURCE
                value: ci-test
              - name: NCCL_CUMEM_HOST_ENABLE
                value: "0"
              - name: HF_HOME
                value: {{ hf_home }}
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: token
        nodeSelector:
          node.kubernetes.io/instance-type: gpu-h200-sxm
        volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  {% elif step.gpu == "h200" %}
   - docker#v5.2.0:
      image: {{ image }}
      always-pull: true
      propagate-environment: true
      gpus: all
      command: ["bash", "{% if fail_fast == "true" %}-xce{% else %}-xc{% endif %}", "(command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1 && cd {{ (step.working_dir or default_working_dir) | safe }} && {{ add_docker_pytest_coverage(step, cov_enabled) }}"]
      environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/benchmark-hf-cache
        - HF_TOKEN
        - CODECOV_TOKEN
        {% if step.num_gpus and step.num_gpus == 2 %}
        - CUDA_VISIBLE_DEVICES=1,2
        {% endif %}
        {% if fail_fast == "true" %}
        - PYTEST_ADDOPTS=-x
        {% endif %}
        {% if branch == "main" %}
        - BUILDKITE_ANALYTICS_TOKEN
        {% endif %}
      volumes:
        - /dev/shm:/dev/shm
        - /data/benchmark-hf-cache:/benchmark-hf-cache
        - /data/benchmark-vllm-cache:/root/.cache/vllm
  {% elif step.gpu == "b200" %}
   - docker#v5.2.0:
      image: {{ image }}
      always-pull: true
      propagate-environment: true
      # gpus will be configured by BUILDKITE_PLUGIN_DOCKER_GPUS in per host environment variable.
      # gpus: all
      command: ["bash", "{% if fail_fast == "true" %}-xce{% else %}-xc{% endif %}", "(command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1 && cd {{ (step.working_dir or default_working_dir) | safe }} && {{ add_docker_pytest_coverage(step, cov_enabled) }}"]
      environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/benchmark-hf-cache
        - HF_TOKEN
        - CODECOV_TOKEN
        {% if fail_fast == "true" %}
        - PYTEST_ADDOPTS=-x
        {% endif %}
        {% if branch == "main" %}
        - BUILDKITE_ANALYTICS_TOKEN
        {% endif %}
      volumes:
        - /dev/shm:/dev/shm
        - /data/benchmark-hf-cache:/benchmark-hf-cache
        - /data/benchmark-vllm-cache:/root/.cache/vllm
  {% elif step.gpu == "h100" %}
  - kubernetes:
      podSpec:
        containers:
          - image: {{ image }}
            command:
              - bash -c "{{ '(command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1 && cd ' ~ ((step.working_dir or default_working_dir) | safe) ~ ' && ' ~ (step.command or (step.commands | join(" && ")) | safe) }}"
            resources:
              limits:
                nvidia.com/gpu: {{ step.num_gpus or 1 }}
            volumeMounts:
              - name: devshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: {{ hf_home }}
            env:
              - name: VLLM_USAGE_SOURCE
                value: ci-test
              - name: NCCL_CUMEM_HOST_ENABLE
                value: "0"
              - name: HF_HOME
                value: {{ hf_home }}
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: token
        nodeSelector:
          node.kubernetes.io/instance-type: gpu-h100-sxm
        volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  {% else %}
  - kubernetes:
      podSpec:
        priorityClassName: ci
        containers:
          - image: {{ image }}
            command:
              - bash -c "{{ '(command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1 && cd ' ~ ((step.working_dir or default_working_dir) | safe) ~ ' && ' ~ (step.command or (step.commands | join(" && ")) | safe) }}"
            resources:
              limits:
                nvidia.com/gpu: {{ step.num_gpus or 1 }}
            volumeMounts:
              - name: devshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: {{ hf_home }}
            env:
              - name: VLLM_USAGE_SOURCE
                value: ci-test
              - name: NCCL_CUMEM_HOST_ENABLE
                value: "0"
              - name: HF_HOME
                value: {{ hf_home }}
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: token
        nodeSelector:
          nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
        volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: {{ hf_home }}
              type: Directory
  {% endif %}
{% endif %}
{% endmacro %}


steps:
  - label: ":docker: build image"
    key: image-build
    depends_on: ~
    timeout_in_minutes: 600
    agents:
      {% if branch == "main" %}
      queue: cpu_queue_postmerge_us_east_1
      {% else %}
      queue: cpu_queue_premerge_us_east_1
      {% endif %}
    commands:
      - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
      - "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 936637512419.dkr.ecr.us-east-1.amazonaws.com"
      - "curl -sSfL https://raw.githubusercontent.com/vllm-project/ci-infra/{{ vllm_ci_branch }}/buildkite/scripts/ci-bake.sh | bash -s -- test-ci"
    env:
      VLLM_CI_BRANCH: "{{ vllm_ci_branch }}"
      CI_HCL_URL: "https://raw.githubusercontent.com/vllm-project/ci-infra/{{ vllm_ci_branch }}/docker/ci.hcl"
      IMAGE_TAG: "{{ docker_image }}"
      {% if branch == "main" %}
      IMAGE_TAG_LATEST: "{{ docker_image_latest }}"
      {% endif %}
      CACHE_TO: "{{ cache_to }}"
      CACHE_FROM: "{{ cache_from }}"
      CACHE_FROM_BASE: "{{ cache_from_base_branch }}"
      CACHE_FROM_MAIN: "{{ cache_from_main }}"
      VLLM_USE_PRECOMPILED: "{{ vllm_use_precompiled | default('0') }}"
      {% if vllm_use_precompiled == "1" %}
      VLLM_MERGE_BASE_COMMIT: "{{ vllm_merge_base_commit }}"
      {% endif %}
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 2
        - exit_status: -10  # Agent was lost
          limit: 2

  - label: ":docker: build image CPU"
    key: image-build-cpu
    depends_on: ~
    agents:
      {% if branch == "main" %}
      queue: cpu_queue_postmerge_us_east_1
      {% else %}
      queue: cpu_queue_premerge_us_east_1
      {% endif %}
    commands:
      - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
      - |
        #!/bin/bash
        if [[ -z $(docker manifest inspect {{ docker_image_cpu }}) ]]; then
          echo "Image not found, proceeding with build..."
        else
          echo "Image found"
          exit 0
        fi
      - "docker build --file docker/Dockerfile.cpu --build-arg max_jobs=16 --build-arg buildkite_commit=$BUILDKITE_COMMIT --build-arg VLLM_CPU_AVX512BF16=true --build-arg VLLM_CPU_AVX512VNNI=true --build-arg VLLM_CPU_AMXBF16=true --tag {{ docker_image_cpu }} --target vllm-test --progress plain ."
      - "docker push {{ docker_image_cpu }}"
    env:
      DOCKER_BUILDKIT: "1"
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 2
        - exit_status: -10  # Agent was lost
          limit: 2

  {% if branch == "main" %}
  - label: ":docker: build image HPU"
    key: image-build-hpu
    depends_on: ~
    soft_fail: true
    agents:
      queue: cpu_queue_postmerge_us_east_1
    commands:
      - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
      - |
        #!/bin/bash
        if [[ -z $(docker manifest inspect {{ docker_image_hpu }}) ]]; then
          echo "Image not found, proceeding with build..."
        else
          echo "Image found"
          exit 0
        fi
      - |
        docker build \
          --file tests/pytorch_ci_hud_benchmark/Dockerfile.hpu \
          --build-arg max_jobs=16 \
          --build-arg buildkite_commit=$BUILDKITE_COMMIT \
          --tag {{ docker_image_hpu }} \
          --progress plain \
          https://github.com/vllm-project/vllm-gaudi.git

        docker push {{ docker_image_hpu }}
    env:
      DOCKER_BUILDKIT: "1"
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 2
        - exit_status: -10  # Agent was lost
          limit: 2
  {% endif %}

  - label: ":docker: build image ARM64 CPU"
    key: image-build-arm64-cpu
    depends_on: ~
    soft_fail: true
    agents:
      {% if branch == "main" %}
      queue: arm64_cpu_queue_postmerge
      {% else %}
      queue: arm64_cpu_queue_premerge
      {% endif %}
    commands:
      - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
      - |
        #!/bin/bash
        if [[ -z $(docker manifest inspect {{ docker_image_arm64_cpu }}) ]]; then
          echo "Image not found, proceeding with build..."
        else
          echo "Image found"
          exit 0
        fi
      - "docker build --file docker/Dockerfile.cpu --build-arg max_jobs=16 --build-arg buildkite_commit=$BUILDKITE_COMMIT --tag {{ docker_image_arm64_cpu }} --target vllm-test --progress plain ."
      - "docker push {{ docker_image_arm64_cpu }}"
    env:
      DOCKER_BUILDKIT: "1"
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 2
        - exit_status: -10  # Agent was lost
          limit: 2

  {% for step in steps %}
  {% if step.fast_check_only != true %}

  {% set ns = namespace(blocked=1) %}

  {% if run_all == "1" %}
  {% set ns.blocked = 0 %}
  {% endif %}

  {% if nightly == "1" %}
  {% set ns.blocked = 0 %}
  {% endif %}

  {% if step.source_file_dependencies %}
    {% for source_file in step.source_file_dependencies %}
      {% for file in list_file_diff %}
        {% if source_file in file %}
          {% set ns.blocked = 0 %}
        {% endif %}
      {% endfor %}
    {% endfor %}
  {% else %}
    {% set ns.blocked = 0 %}
  {% endif %}

  {% if (ns.blocked == 1 or (step.optional and nightly != "1")) and not (step.autorun_on_main == true and branch == "main") %}
  - block: "Run {{ step.label }}"
    depends_on: image-build
    key: block-{{ step.label | replace(" ", "-") | lower | replace("(", "") | replace(")", "") | replace("%", "") | replace(",", "-") | replace("+", "-") }}
  {% endif %}

  - label: "{{ step.label }}"
    {% if (ns.blocked == 1 or (step.optional and nightly != "1")) and not (step.autorun_on_main == true and branch == "main") %}
    depends_on: block-{{ step.label | replace(" ", "-") | lower | replace("(", "") | replace(")", "") | replace("%", "") | replace(",", "-") | replace("+", "-") }}
    {% else %}
    depends_on: {{ "image-build-cpu" if step.no_gpu else "image-build" }}
    {% endif %}
    {% if step.label == "Python-only Installation Test" %}
    soft_fail: true
    {% else %}
    soft_fail: {{ step.soft_fail or false }}
    {% endif %}
    {{ render_cuda_config(step, docker_image_cpu if step.no_gpu else (pull_through_docker_image if step.gpu == "h100" else docker_image), default_working_dir, hf_home_fsx, hf_home, branch) | indent(4, true) }}
  {% endif %}
  {% endfor %}

  - group: "vLLM Against PyTorch Nightly"
    depends_on: ~
    steps:
      {% if nightly != "1" %}
      - block: Build torch nightly image
        key: block-build-torch-nightly
        depends_on: ~
      {% endif %}
      - label: ":docker: build image torch nightly"
        key: image-build-torch-nightly
        {% if nightly != "1" %}
        depends_on: block-build-torch-nightly
        {% else %}
        depends_on: ~
        {% endif %}
        soft_fail: true
        agents:
          {% if branch == "main" %}
          queue: cpu_queue_postmerge_us_east_1
          {% else %}
          queue: cpu_queue_premerge_us_east_1
          {% endif %}
        timeout_in_minutes: 600
        commands:
          - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
          - "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 936637512419.dkr.ecr.us-east-1.amazonaws.com"
          - "docker buildx create --name vllm-builder --driver docker-container --use"
          - "docker buildx inspect --bootstrap"
          - "docker buildx ls"
          - |
            #!/bin/bash
            if [[ -z $(docker manifest inspect {{ docker_image_torch_nightly }}) ]]; then
              echo "Image not found, proceeding with build..."
            else
              echo "Image found"
              exit 0
            fi
          - >
            docker buildx build --file docker/Dockerfile
            --build-arg max_jobs=16
            --build-arg buildkite_commit=$BUILDKITE_COMMIT
            --build-arg USE_SCCACHE=1
            --build-arg PYTORCH_NIGHTLY=1
            --build-arg TORCH_CUDA_ARCH_LIST="8.0 8.9 9.0 10.0"
            --build-arg FI_TORCH_CUDA_ARCH_LIST="8.0 8.9 9.0a 10.0a"
            --tag {{ docker_image_torch_nightly }}
            --push
            --target test
            --progress plain .
        env:
          DOCKER_BUILDKIT: "1"
        retry:
          automatic:
            - exit_status: -1  # Agent was lost
              limit: 2
            - exit_status: -10  # Agent was lost
              limit: 2
    {% for step in steps %}
    {% if step.torch_nightly %}
      {% set ns = namespace(blocked=1) %}
      {% if nightly == "1" %}
      {% set ns.blocked = 0 %}
      {% endif %}

      {% if step.source_file_dependencies %}
        {% for source_file in step.source_file_dependencies %}
          {% for file in list_file_diff %}
            {% if source_file in file %}
              {% set ns.blocked = 0 %}
            {% endif %}
          {% endfor %}
        {% endfor %}
      {% else %}
        {% set ns.blocked = 0 %}
      {% endif %}

      {% if ns.blocked == 1 or (step.optional and nightly != "1") %}
      - block: "Run Torch Nightly {{ step.label }}"
        depends_on: image-build-torch-nightly
        key: block-torch-nightly-{{ step.label | replace(" ", "-") | lower | replace("(", "") | replace(")", "") | replace("%", "") | replace(",", "-") | replace("+", "-") }}
      {% endif %}

      - label: "Torch Nightly {{ step.label }}"
        {% if ns.blocked == 1 or (step.optional and nightly != "1") %}
        depends_on: block-torch-nightly-{{ step.label | replace(" ", "-") | lower | replace("(", "") | replace(")", "") | replace("%", "") | replace(",", "-") | replace("+", "-") }}
        {% else %}
        depends_on: image-build-torch-nightly
        {% endif %}
        soft_fail: true
        {{ render_cuda_config(step, docker_image_torch_nightly, default_working_dir, hf_home_fsx, hf_home, branch) | indent(8, true) }}
      {% endif %}
      {% endfor %}

  - group: "AMD Tests"
    depends_on: ~
    steps:
      - label: "AMD: :docker: build image"
        depends_on: ~
        soft_fail: false
        commands:
          # Handle the introduction of test target in Dockerfile.rocm
          - >
            docker build
            --build-arg max_jobs=16
            --build-arg REMOTE_VLLM=1
            --build-arg ARG_PYTORCH_ROCM_ARCH='gfx90a;gfx942'
            --build-arg VLLM_BRANCH=$BUILDKITE_COMMIT
            --tag {{ docker_image_amd }}
            -f docker/Dockerfile.rocm
            --target test
            --no-cache
            --progress plain .
          - "docker push {{ docker_image_amd }}"
        key: "amd-build"
        env:
          DOCKER_BUILDKIT: "1"
        retry:
          automatic:
            - exit_status: -1  # Agent was lost
              limit: 1
            - exit_status: -10  # Agent was lost
              limit: 1
            - exit_status: 1  # Machine occasionally fail
              limit: 1
        agents:
          queue: amd-cpu

    {% for step in steps %}
    {% if step.mirror_hardwares and mirror_hw in step.mirror_hardwares %}
      - label: "AMD MI300: {{ step.label }}"
        depends_on: amd-build
        soft_fail: true
        agents:
         {% if step.label and step.label=="Benchmarks" or step.label=="Kernels Attention Test %N" or step.label=="LoRA Test %N" or step.label=="Kernels Quantization Test %N" %}
           queue: amd_mi325_8
         {% elif step.label=="Distributed Tests (4 GPUs)" or step.label=="2 Node Tests (4 GPUs in total)" or step.label=="Multi-step Tests (4 GPUs)" or step.label=="Pipeline Parallelism Test" or step.label=="LoRA TP Test (Distributed)" %}
           queue: amd_mi325_4
         {% elif step.label=="Distributed Comm Ops Test" or step.label=="Distributed Tests (2 GPUs)" or step.label=="Plugin Tests (2 GPUs)" or step.label=="Weight Loading Multiple GPU Test" or step.label=="Weight Loading Multiple GPU Test - Large Models" %}
           queue: amd_mi325_2
         {% else %}
           queue: amd_mi325_1
         {% endif%}
        command: bash .buildkite/scripts/hardware_ci/run-amd-test.sh "(command rocm-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1 && cd {{ (step.working_dir or default_working_dir) | safe  }} ; {{ step.command  or (step.commands | join(" && ")) | safe }}"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        {% endif %}
    {% endfor %}

  - label: "Neuron Test"
    depends_on: ~
    agents:
      queue: neuron
    command: bash .buildkite/scripts/hardware_ci/run-neuron-test.sh
    soft_fail: true

  - block: "Run Intel CPU test"
    depends_on: ~
    key: block-intel-cpu

  - label: "Intel CPU Test"
    {% if branch == "main" %}
    depends_on: ~
    {% else %}
    depends_on: block-intel-cpu
    {% endif %}
    soft_fail: true
    agents:
      queue: intel-cpu
    command: bash .buildkite/scripts/hardware_ci/run-cpu-test.sh

  - label: "Intel HPU Test"
    depends_on: ~
    soft_fail: true
    agents:
      queue: intel-hpu
    command: bash .buildkite/scripts/hardware_ci/run-hpu-test.sh

  - label: "Intel GPU Test"
    soft_fail: true
    depends_on: ~
    agents:
      queue: intel-gpu
    command: bash .buildkite/scripts/hardware_ci/run-xpu-test.sh

  - block: "Run Arm CPU test"
    depends_on: ~
    key: block-arm-cpu

  - label: "Arm CPU Test"
    {% if branch == "main" %}
    depends_on: ~
    {% else %}
    depends_on: block-arm-cpu
    {% endif %}
    soft_fail: true
    agents:
      queue: arm-cpu
    command: bash .buildkite/scripts/hardware_ci/run-cpu-test-arm.sh

  - label: "Ascend NPU Test"
    soft_fail: true
    depends_on: ~
    timeout_in_minutes: 20
    agents:
      queue: "ascend"
    command: bash .buildkite/scripts/hardware_ci/run-npu-test.sh

  {% if branch == "main" %}
  - block: "Run IBM Power CPU test"
    key: block-ibm-power
    depends_on: ~

  - label: "IBM Power(ppc64le) CPU Test"
    depends_on: block-ibm-power
    key: ibm-ppc64-test
    soft_fail: true
    agents:
      queue: ibm-ppc64le
    command: bash .buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh

  - label: "IBM Power(ppc64le) Build Failure Notification"
    depends_on: ibm-ppc64-test
    soft_fail: true
    agents:
      queue: ibm-ppc64le
    commands: |
      if [ $$(buildkite-agent step get "outcome" --step "IBM Power(ppc64le) CPU Test") != "passed" ]; then
         cat <<- YAML | buildkite-agent pipeline upload
         steps:
           - label: "Notify owners about failing test"
             soft_fail: true
             agents:
               queue: ibm-ppc64le
             command: echo "IBM Power(ppc64le) Build/Test failed"
             notify:
               - slack:
                   channels:
                     - "vllm#vllm-ci-on-power"
      YAML
      fi  {% else %}
  - block: "Run IBM Power(ppc64le) CPU Test"
    depends_on: ~
    key: block-ibm-ppc64-test

  - label: "IBM Power(ppc64le) CPU Test"
    depends_on: block-ibm-ppc64-test
    soft_fail: true
    agents:
      queue: ibm-ppc64le
    command: bash .buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh
  {% endif %}

  {% if nightly != "1" %}
  - block: Run "IBM Z (s390x) CPU Test"
    depends_on: ~
    key: block-ibm-s390x
  {% endif %}

  - label: "IBM Z (s390x) CPU Test"
    {% if nightly == "1" %}
    depends_on: ~
    {% else %}
    depends_on: block-ibm-s390x
    {% endif %}
    soft_fail: true
    agents:
      queue: ibm_s390x
    command: bash .buildkite/scripts/hardware_ci/run-cpu-test-s390x.sh

  {% if nightly == "1" %}
  - label: "GH200 Test"
    depends_on: ~
    soft_fail: true
    agents:
      queue: gh200_queue
    command: nvidia-smi && bash .buildkite/scripts/hardware_ci/run-gh200-test.sh
  {% endif %}
